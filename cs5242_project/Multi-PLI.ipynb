{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ligands = pd.read_csv(\"dataset_20220217_2/ligand.csv\")\n",
    "df_centroids = pd.read_csv(\"dataset_20220217_2/centroids.csv\")\n",
    "df_pair = pd.read_csv(\"dataset_20220217_2/pair.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smiles string - One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define SMILES characters ----------------------------------------------------\n",
    "SMILES_CHARS = [' ',\n",
    "                '#', '%', '(', ')', '+', '-', '.', '/',\n",
    "                '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "                '=', '@',\n",
    "                'A', 'B', 'C', 'F', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P',\n",
    "                'R', 'S', 'T', 'V', 'W', 'X', 'Z',\n",
    "                '[', '\\\\', ']',\n",
    "                'a', 'b', 'c', 'e', 'g', 'i', 'l', 'n', 'o', 'p', 'r', 's',\n",
    "                't', 'u']\n",
    "                \n",
    "# define encoder and decoder --------------------------------------------------\n",
    "smi2index = dict( (c,i) for i,c in enumerate( SMILES_CHARS ) )\n",
    "index2smi = dict( (i,c) for i,c in enumerate( SMILES_CHARS ) )\n",
    "\n",
    "def smiles_encoder( smiles, maxlen=200 ):\n",
    "    X = np.zeros( ( maxlen, len( SMILES_CHARS ) ) )\n",
    "    for i, c in enumerate( smiles ):\n",
    "        X[i, smi2index[c] ] = 1\n",
    "    return X\n",
    "\n",
    "def smiles_decoder( X ):\n",
    "    smi = ''\n",
    "    X = X.argmax( axis=-1 )\n",
    "    for i in X:\n",
    "        smi += index2smi[ i ]\n",
    "    return smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMISET = {\"C\": 67, \"l\": 1, \".\": 2, \"c\": 3, \"1\": 4, \"2\": 5, \"(\": 6,\n",
    "          \"N\": 7, \"=\": 8, \"3\": 9, \")\": 10, \"n\": 11, \"[\": 12, \"H\": 13,\n",
    "           \"]\": 14, \"O\": 15, \"@\": 16, \"s\": 17, \"+\": 18, \"/\": 19, \"S\": 20,\n",
    "            \"F\": 21, \"-\": 22, \"4\": 23, \"B\": 24, \"r\": 25, \"o\": 26, \"\\\\\": 27,\n",
    "             \"#\": 28, \"5\": 29, \"a\": 30, \"P\": 31, \"e\": 32, \"6\": 33, \"7\": 34,\n",
    "              \"I\": 35, \"A\": 36, \"i\": 37, \"8\": 38, \"9\": 39, \"Z\": 40, \"K\": 41,\n",
    "               \"L\": 42, \"%\": 43, \"0\": 44, \"T\": 45, \"g\": 46, \"G\": 47, \"d\": 48,\n",
    "                \"M\": 49, \"b\": 50, \"u\": 51, \"t\": 52, \"R\": 53, \"p\": 54, \"m\": 55,\n",
    "                 \"W\": 56, \"Y\": 57, \"V\": 58, \"~\": 59, \"U\": 60, \"E\": 61, \"f\": 62,\n",
    "                  \"X\": 63, \"D\": 64, \"y\": 65, \"h\": 66}\n",
    "\n",
    "PROTSET = {\"A\": 1, \"R\": 2, \"N\": 3, \"D\": 4, \"C\": 5, \"Q\": 6,\n",
    "           \"E\": 7, \"G\": 8, \"H\": 9, \"I\": 10, \"L\": 11, \"K\": 12,\n",
    "           \"M\": 13, \"F\": 14, \"P\": 15, \"S\": 16, \"T\": 17, \"W\": 18,\n",
    "           \"Y\": 19, \"V\": 20, \"O\": 21}\n",
    "\n",
    "pro_missing_ls = []\n",
    "\n",
    "def one_hot_smiles(line, MAX_SMI_LEN=200):\n",
    "    X = np.zeros((1, MAX_SMI_LEN, len(SMISET)))  # +1\n",
    "\n",
    "    if type(line)!=str:\n",
    "        print('SMILE format is not str!')\n",
    "    for i, ch in enumerate(line[:MAX_SMI_LEN]):\n",
    "        tmp=SMISET.get(ch)\n",
    "        if tmp:\n",
    "            X[0, i, tmp - 1] = 1\n",
    "        else:\n",
    "            print(line,'exits not in SMISET character',ch)\n",
    "#     X = X.tolist() \n",
    "    return X\n",
    "\n",
    "def one_hot_protein(line, MAX_SEQ_LEN=1200):\n",
    "    X = np.zeros((1, MAX_SEQ_LEN, len(PROTSET)))\n",
    "    for i, ch in enumerate(line[:MAX_SEQ_LEN]):\n",
    "        tmp=PROTSET.get(ch)\n",
    "        if tmp:\n",
    "            X[0, i, tmp - 1] = 1\n",
    "        else:\n",
    "#             print('exits not in PROTSET character',ch)\n",
    "            if ch not in pro_missing_ls:\n",
    "                pro_missing_ls.append(ch)\n",
    "#     X = X.tolist()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "mypath = os.path.join(os.getcwd(), \"dataset_20220217_2\", \"pdbs\")\n",
    "pdb_files_dic = {f: os.path.join(mypath, f) for f in listdir(mypath) if isfile(join(mypath, f))}\n",
    "\n",
    "unique_atom_dic = {}\n",
    "\n",
    "for i in pdb_files_dic.keys():\n",
    "    path = pdb_files_dic[i]\n",
    "    X_list, Y_list, Z_list, atomtype_list = read_pdb(path)\n",
    "    \n",
    "    curr_unique = list(set(atomtype_list))\n",
    "    for i in curr_unique:\n",
    "        if i not in unique_atom_dic.keys():\n",
    "            unique_atom_dic[i] = 1\n",
    "        else: \n",
    "            unique_atom_dic[i] = unique_atom_dic[i]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_smile_dic = {}\n",
    "\n",
    "for i in df_ligands[\"Smiles\"]:\n",
    "    curr_unique = list(i)\n",
    "    for i in curr_unique:\n",
    "        if i not in unique_smile_dic.keys():\n",
    "            unique_smile_dic[i] = 1\n",
    "        else: \n",
    "            unique_smile_dic[i] = unique_smile_dic[i]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "for i in unique_smile_dic.keys():\n",
    "    if i not in SMISET.keys():\n",
    "        print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import periodictable as pt\n",
    "unique_atoms = {'C': pt.C.number, \n",
    "                'N': pt.N.number,  \n",
    "                'O': pt.O.number, \n",
    "                'S': pt.S.number,\n",
    "               'P': pt.P.number}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdb(filename: str):\n",
    "    \"\"\"Read a protein file to get four atom information lists.\n",
    "    \n",
    "    You can copy this function to your project code.\n",
    "    \"\"\" \n",
    "    with open(filename, 'r') as file:\n",
    "        strline_L = file.readlines()\n",
    "    strline_L=[strline.strip() for strline in strline_L]\n",
    "\n",
    "    X_list=[float(strline.split()[-3]) for strline in strline_L]\n",
    "    Y_list=[float(strline.split()[-2]) for strline in strline_L]\n",
    "    Z_list=[float(strline.split()[-1]) for strline in strline_L]\n",
    "    atomtype_list=[strline.split()[-7][0] for strline in strline_L]\n",
    "\n",
    "    return X_list, Y_list, Z_list, atomtype_list\n",
    "\n",
    "def convert_protein_list_to_matrix(X_list, Y_list, Z_list):\n",
    "    protein_coords = np.concatenate((np.array(X_list).reshape(-1, 1), \n",
    "                                     np.array(Y_list).reshape(-1, 1), \n",
    "                                     np.array(Z_list).reshape(-1, 1)), \n",
    "                                    axis = 1)\n",
    "    return protein_coords\n",
    "\n",
    "def find_closet_k_atoms(atomtype_list, complex_coords, atom_index, k):\n",
    "    target_coords = complex_coords[atom_index]\n",
    "    euclidean_dist = np.sum(np.square(complex_coords - target_coords), axis = 1)\n",
    "    closest_k_index = np.argsort(euclidean_dist)[:k]\n",
    "    \n",
    "    if atom_index in closest_k_index:\n",
    "        closest_k_index_updated = [j for j in closest_k_index if j!=atom_index]\n",
    "        closest_k_index_updated.append(np.argsort(euclidean_dist)[k])\n",
    "    else:\n",
    "        closest_k_index_updated = closest_k_index\n",
    "    \n",
    "    closest_k_dist = euclidean_dist[closest_k_index_updated]\n",
    "    closest_k_coords = complex_coords[closest_k_index_updated]\n",
    "    closest_k_atoms = np.array(atomtype_list)[closest_k_index_updated]\n",
    "    closest_k_atoms_num = np.array([unique_atoms[i] for i in closest_k_atoms])\n",
    "    \n",
    "    return closest_k_atoms.reshape(1,k), closest_k_atoms_num.reshape(1,k), closest_k_dist.reshape(1,k), closest_k_coords\n",
    "\n",
    "def featurize_pdb(path, k, max_length):\n",
    "    X_list, Y_list, Z_list, atomtype_list = read_pdb(path)\n",
    "    curr_protein_coords = convert_protein_list_to_matrix(X_list, Y_list, Z_list)\n",
    "    output_dist = np.zeros([max_length, k])\n",
    "    output_atoms = []\n",
    "    output_atoms_num = np.zeros([max_length, k])\n",
    "    for i in range(curr_protein_coords.shape[0]):\n",
    "        k_atoms, k_atoms_num, k_dist, k_coords = find_closet_k_atoms(atomtype_list,\n",
    "                                                                     curr_protein_coords,\n",
    "                                                                     i,\n",
    "                                                                     k)\n",
    "        output_atoms.append(list(k_atoms)[0])\n",
    "        output_atoms_num[i] = k_atoms_num\n",
    "        output_dist[i] = k_dist\n",
    "    \n",
    "    return output_dist, output_atoms, output_atoms_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dist, output_atoms, output_atoms_num = featurize_pdb('/Users/user/Desktop/CS5242/project/cs5242_project/dataset_20220217_2/pdbs/102D.pdb', 12, 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.061981,  5.776022,  6.73325 , ...,  6.      ,  6.      ,\n",
       "         6.      ],\n",
       "       [ 2.061981,  2.253537,  5.413489, ...,  6.      , 15.      ,\n",
       "         6.      ],\n",
       "       [ 2.081778,  2.253537,  2.412322, ..., 15.      ,  8.      ,\n",
       "         6.      ],\n",
       "       ...,\n",
       "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((output_dist, output_atoms_num), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in tqdm(pdb_files_dic.keys()):\n",
    "    path = pdb_files_dic[j]\n",
    "#     print(path)\n",
    "    X_list, Y_list, Z_list, atomtype_list = read_pdb(path)\n",
    "    curr_protein_coords = convert_protein_list_to_matrix(X_list, Y_list, Z_list)\n",
    "    output_dist = np.zeros([curr_protein_coords.shape[0], k])\n",
    "    output_atoms = []\n",
    "    for i in range(curr_protein_coords.shape[0]):\n",
    "        k_atoms, k_dist, k_coords = find_closet_k_atoms(atomtype_list, \n",
    "                                                        curr_protein_coords,\n",
    "                                                        i,\n",
    "                                                        k)\n",
    "        output_atoms.append(list(k_atoms)[0])\n",
    "        output_dist[i] = k_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_molecule_features(x_list: list, y_list: list, z_list: list, atom_type_list: list,\n",
    "                            molecule_is_protein: bool) -> np.array:\n",
    "    \"\"\"\n",
    "    Convert the data extract from file into a np.ndarray.\n",
    "    The information of one atom is represented as a line in the array.\n",
    "    See settings.py for values used to represented categorical features (molecule type and atom type)\n",
    "    :param x_list: list of x coordinates\n",
    "    :param y_list: list of y coordinates\n",
    "    :param z_list: list of z coordinates\n",
    "    :param atom_type_list: list of atom type (string)\n",
    "    :param molecule_is_protein: boolean\n",
    "    :return: np.ndarray of dimension (nb_atoms, 3 + nb_atom_features)\n",
    "    \"\"\"\n",
    "    nb_atoms = len(x_list)\n",
    "\n",
    "    # One hot encoding for atom type and molecule types\n",
    "    is_hydrophobic_list = np.array([1. if atom_type in HYDROPHOBIC_TYPES else 0. for atom_type in atom_type_list])\n",
    "    is_polar_list = 1. - is_hydrophobic_list\n",
    "\n",
    "#     is_from_protein_list = (1. * molecule_is_protein) * np.ones((nb_atoms,))\n",
    "#     is_from_ligand_list = 1. - is_from_protein_list\n",
    "\n",
    "    # See `FEATURES_NAMES` in settings to see how the features are organized\n",
    "    molecule_features = np.array([x_list, y_list, z_list,\n",
    "                                  is_hydrophobic_list, is_polar_list]).T\n",
    "\n",
    "\n",
    "    return molecule_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_negative_example(df_pairs, df_ligands, ratio, seed):\n",
    "    proteins_ls = list(df_pairs[\"PID\"])\n",
    "    ligands_ls = list(df_ligands[\"LID\"])\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    out_proteins_ls = []\n",
    "    out_ligands_ls = []\n",
    "    target_ls = []\n",
    "    \n",
    "    for i in proteins_ls:\n",
    "        paired_ligand = df_pairs[df_pairs[\"PID\"] == i][\"LID\"].values[0]\n",
    "        for j in range(ratio):\n",
    "            out_proteins_ls.append(i)\n",
    "            chosen_ligand = np.random.choice([k for k in ligands_ls if k!=paired_ligand], 1)[0] \n",
    "            out_ligands_ls.append(chosen_ligand)\n",
    "            target_ls.append(0)\n",
    "    \n",
    "    df_out = pd.DataFrame({\"PID\": out_proteins_ls,\n",
    "                          \"LID\": out_ligands_ls,\n",
    "                          \"target\": target_ls})\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_positive = len(df_pair)\n",
    "df_positive = df_pair.copy()\n",
    "df_positive[\"target\"] = 1\n",
    "df_train_positive = df_positive.iloc[0:int(np.floor(num_positive * 0.7)), :]\n",
    "df_validation_positive = df_positive.iloc[int(np.floor(num_positive * 0.7)):, :]\n",
    "df_train_negative = generate_negative_example(df_train_positive, df_ligands, 2, 0)\n",
    "df_validation_negative = generate_negative_example(df_validation_positive, df_ligands, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train_positive, df_train_negative])\n",
    "df_test = pd.concat([df_validation_positive, df_validation_negative])\n",
    "df_train.reset_index(inplace=True)\n",
    "df_test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_PDB(pid, pdbs_dir):\n",
    "    X_list, Y_list, Z_list, atomtype_list = read_pdb(path)\n",
    "    return one_hot_protein(atomtype_list)\n",
    "\n",
    "def batch_process_SMILE(ligands):\n",
    "    return one_hot_smiles(ligands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To add a regularization term for the weight parameter, you could manually add it to the loss:\n",
    "output = model(input)\n",
    "loss = criterion(output, target)\n",
    "loss = loss + torch.norm(model.layer.weight, p=2)\n",
    "\n",
    "intialize weight:\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "model.apply(weight_init)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_conv2d = nn.Conv2d(1, 8, kernel_size = (1, 1), stride = 1, padding=\"same\", bias = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(trainloader, 0):\n",
    "    if i == 0:\n",
    "        ligand = data[0].to(device)\n",
    "        protein = data[1].to(device)\n",
    "        target = data[2].to(device)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 1200, 21])"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 8, 1200, 21])"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv2d(protein.float()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inception block\n",
    "class Conv2dLayer(nn.Module):\n",
    "    def __init__(self, out_channels, num_row, num_col,\n",
    "              padding='same', strides=1, use_bias=False, in_channels=1):\n",
    "        super().__init__()\n",
    "        print (f\"current out channels: {out_channels}\")\n",
    "        self.net = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size = (num_row, num_col),\n",
    "              stride = strides, padding=padding, bias = use_bias),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        output = self.net(x)\n",
    "        print (f\"conv2d current output {x.shape}\")\n",
    "        return output\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, filters_1x1, filters_3x3_reduce, filters_3x3,\n",
    "                    filters_5x5_reduce, filters_5x5, filters_pool_proj, in_channels = 1):\n",
    "        super().__init__()\n",
    "        self.layer0 = Conv2dLayer(filters_1x1, 1, 1, in_channels = in_channels)\n",
    "        self.layer1 = Conv2dLayer(filters_3x3_reduce, 1, 1, in_channels = in_channels)\n",
    "        self.layer2 = Conv2dLayer(filters_3x3, 3, 3, in_channels = filters_3x3_reduce)\n",
    "        self.layer3 = Conv2dLayer(filters_5x5_reduce, 1, 1, in_channels = in_channels)\n",
    "        self.layer4 = Conv2dLayer(filters_5x5, 3, 3, in_channels = filters_5x5_reduce)\n",
    "        self.layer5 = Conv2dLayer(filters_5x5, 3, 3, in_channels = filters_5x5)\n",
    "        self.layer6 = nn.MaxPool2d(kernel_size=3, stride=2, padding=\"same\")\n",
    "        self.layer7 = Conv2dLayer(filters_pool_proj, 1, 1, in_channels = in_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        branch_0 = self.layer0(x)\n",
    "        branch_1 = self.layer1(x)\n",
    "        branch_1 = self.layer2(branch_1)\n",
    "        branch_2 = self.layer3(x)\n",
    "        branch_2 = self.layer4(branch_2)\n",
    "        branch_2 = self.layer5(branch_2)\n",
    "        branch_3 = self.layer6(x)\n",
    "        branch_3 = self.layer7(branch_3)\n",
    "        \n",
    "        x_out = torch.cat([branch_0, branch_1, branch_2, branch_3], dim=1)\n",
    "        return x_out\n",
    "\n",
    "class InceptionBlockB(nn.Module):\n",
    "    def __init__(self, filters_1x1, filters_5x5_reduce, filters_5x5,\n",
    "                      filters_7x7_reduce, filters_1x7,filters_7x1,filters_pool_proj):\n",
    "        super().__init__()\n",
    "        self.layer0 = Conv2dLayer(filters_1x1, 1, 1)\n",
    "        self.layer1 = Conv2dLayer(filters_7x7_reduce, 1, 1)\n",
    "        self.layer2 = Conv2dLayer(filters_1x7, 1, 7)\n",
    "        self.layer3 = Conv2dLayer(filters_7x1, 7, 1)\n",
    "        self.layer4 = Conv2dLayer(filters_5x5_reduce, 1, 1)\n",
    "        self.layer5 = Conv2dLayer(filters_5x5, 3, 3)\n",
    "        self.layer6 = nn.AvgPool2d(kernel_size=3, stride=1, padding=\"same\")\n",
    "        self.layer7 = Conv2dLayer(filters_pool_proj, 1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        branch_0 = self.layer0(x)\n",
    "        branch_1 = self.layer1(x)\n",
    "        branch_1 = self.layer2(branch_1)\n",
    "        branch_1 = self.layer3(branch_1)\n",
    "        branch_2 = self.layer4(x)\n",
    "        branch_2 = self.layer5(branch_2)\n",
    "        branch_2 = self.layer5(branch_2)\n",
    "        branch_3 = self.layer6(x)\n",
    "        branch_3 = self.layer7(branch_3)\n",
    "        \n",
    "        x_out = torch.cat([branch_0, branch_1, branch_2, branch_3], dim=1)\n",
    "        return x_out\n",
    "\n",
    "class SimpleBlock(nn.Module):\n",
    "    def __init__(self, nb_filter, num_row, num_col):\n",
    "        self.layer = Conv2dLayer(nb_filter, num_row, num_col)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "# class Attention3DBlock(nn.Module):\n",
    "#     def __init__(self, hidden_states,input_shape1, input_shape2, out_shape=128):\n",
    "#         self.dense1 = nn.linear(input_shape1, out_shape, bias=False) # identify input_shape here\n",
    "#         self.softmax = nn.Softmax()\n",
    "#         self.dense2 = nn.linear(input_shape2, out_shape) # identify input_shape here\n",
    "#         self.tanh = nn.Tanh()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         score_first_part = self.dense1(x)\n",
    "#         h_t = score_first_part[:, -1, :]\n",
    "#         score = torch.dot(score_first_part, h_t)\n",
    "#         attention_weights = self.softmax(score)\n",
    "#         context_vector = torch.dot(hidden_states, attention_weights)   # how does keras dot work????\n",
    "#         pre_activation = torch.tensordot([context_vector, h_t], dims=([1], [1]))\n",
    "#         attention_vector = self.dense2(pre_activation)\n",
    "        \n",
    "#         return attention_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, alpha, device, pro_branch_switch1='inception_block', pro_branch_switch2='inception_block', \n",
    "                 pro_branch_switch3='inception_block_b', pro_add_attention=False,\n",
    "                comp_branch_switch1='inception_block', comp_branch_switch2='inception_block', \n",
    "                 comp_branch_switch3='inception_block_b', comp_add_attention=False):\n",
    "        super().__init__()\n",
    "        self.pro_branch_switch1 = pro_branch_switch1\n",
    "        self.pro_branch_switch2 = pro_branch_switch2\n",
    "        self.pro_branch_switch3 = pro_branch_switch3\n",
    "        self.pro_add_attention = pro_add_attention\n",
    "        self.comp_branch_switch1 = comp_branch_switch1\n",
    "        self.comp_branch_switch2 = comp_branch_switch2\n",
    "        self.comp_branch_switch3 = comp_branch_switch3\n",
    "        self.comp_add_attention = comp_add_attention\n",
    "        self.alpha = alpha\n",
    "        self.device = device\n",
    "        self._create_network()\n",
    "        self._init_params()\n",
    "        \n",
    "    \n",
    "    def _create_network(self):\n",
    "        protein_blocks = []\n",
    "        if self.pro_branch_switch1 == \"inception_block\":\n",
    "            protein_blocks.append(InceptionBlock(filters_1x1=8, filters_3x3_reduce=1, filters_3x3=32,\n",
    "                                         filters_5x5_reduce=1, filters_5x5=32, filters_pool_proj=16, in_channels=1))\n",
    "        else:\n",
    "            protein_blocks.append(SimpleBlock(nb_filter=32, num_row=3, num_col=3))\n",
    "        protein_blocks.append(nn.MaxPool2d(kernel_size=3, stride=3, padding=\"same\"))\n",
    "        \n",
    "        if self.pro_branch_switch2 == \"inception_block\":\n",
    "            protein_blocks.append(InceptionBlock(filters_1x1=16, filters_3x3_reduce=16, filters_3x3=64,\n",
    "                                         filters_5x5_reduce=16, filters_5x5=64, filters_pool_proj=32))\n",
    "        else:\n",
    "            protein_blocks.append(SimpleBlock(nb_filter=64,num_row=3,num_col=3))\n",
    "        protein_blocks.append(nn.MaxPool2d(kernel_size=3, stride=3, padding=\"same\"))\n",
    "        \n",
    "        if self.pro_branch_switch3 == \"inception_block\":\n",
    "            protein_blocks.append(InceptionBlock(filters_1x1=32, filters_3x3_reduce=64, filters_3x3=128,\n",
    "                                         filters_5x5_reduce=64, filters_5x5=128, filters_pool_proj=64))\n",
    "        elif self.pro_branch_switch3 == \"inception_block_b\":\n",
    "            protein_blocks.append(InceptionBlockB(filters_1x1=32, filters_5x5_reduce=64, filters_5x5=128,\n",
    "                                         filters_7x7_reduce=64, filters_1x7=128,filters_7x1=128, filters_pool_proj=64))\n",
    "        else:\n",
    "            protein_blocks.append(SimpleBlock(nb_filter=128,num_row=3,num_col=3))\n",
    "        protein_blocks.append(nn.MaxPool2d(kernel_size=3, stride=3, padding=\"same\"))\n",
    "        \n",
    "        if self.pro_add_attention:\n",
    "            pass\n",
    "        else:\n",
    "            protein_blocks.append(nn.Flatten(start_dim=1, end_dim=-1))\n",
    "            protein_blocks.append(nn.LazyLinear(1024)) # identify input_shape here\n",
    "            protein_blocks.append(nn.ReLU())\n",
    "        protein_blocks.append(nn.Dropout(self.alpha))\n",
    "        self.protein_blocks = nn.Sequential(*protein_blocks)\n",
    "        \n",
    "        ligand_blocks = []\n",
    "        if self.comp_branch_switch1 == \"inception_block\":\n",
    "            ligand_blocks.append(InceptionBlock(filters_1x1=8, filters_3x3_reduce=1, filters_3x3=16,\n",
    "                                         filters_5x5_reduce=1, filters_5x5=16, filters_pool_proj=16))\n",
    "        else:\n",
    "            ligand_blocks.append(SimpleBlock(nb_filter=32, num_row=3, num_col=3))\n",
    "        ligand_blocks.append(nn.MaxPool2d(kernel_size=2, stride=2, padding=\"same\"))\n",
    "        \n",
    "        if self.comp_branch_switch2 == \"inception_block\":\n",
    "            ligand_blocks.append(InceptionBlock(filters_1x1=16, filters_3x3_reduce=16, filters_3x3=64,\n",
    "                                         filters_5x5_reduce=16, filters_5x5=64, filters_pool_proj=32))\n",
    "        else:\n",
    "            ligand_blocks.append(SimpleBlock(nb_filter=64,num_row=3,num_col=3))\n",
    "        ligand_blocks.append(nn.MaxPool2d(kernel_size=2, stride=2, padding=\"same\"))\n",
    "        \n",
    "        if self.comp_branch_switch3 == \"inception_block\":\n",
    "            ligand_blocks.append(InceptionBlock(filters_1x1=32, filters_3x3_reduce=32, filters_3x3=128,\n",
    "                                         filters_5x5_reduce=32, filters_5x5=128, filters_pool_proj=32))\n",
    "        elif self.comp_branch_switch3 == \"inception_block_b\":\n",
    "            ligand_blocks.append(InceptionBlockB(filters_1x1=32, filters_5x5_reduce=32, filters_5x5=128,\n",
    "                                         filters_7x7_reduce=32, filters_1x7=128,filters_7x1=128, filters_pool_proj=32))\n",
    "        else:\n",
    "            ligand_blocks.append(SimpleBlock(nb_filter=128,num_row=3,num_col=3))\n",
    "        ligand_blocks.append(nn.MaxPool2d(kernel_size=2, stride=2, padding=\"same\"))\n",
    "        \n",
    "        if self.comp_add_attention:\n",
    "            pass\n",
    "        else:\n",
    "            ligand_blocks.append(nn.Flatten(start_dim=1, end_dim=-1))\n",
    "            ligand_blocks.append(nn.LazyLinear(1024)) # identify input_shape here\n",
    "            ligand_blocks.append(nn.ReLU())\n",
    "        ligand_blocks.append(nn.Dropout(self.alpha))\n",
    "        self.ligand_blocks = nn.Sequential(*ligand_blocks)\n",
    "        \n",
    "        combined_blocks = []\n",
    "        combined_blocks.append(nn.Linear(2048, 512))\n",
    "        combined_blocks.append(nn.ReLU())\n",
    "        combined_blocks.append(nn.Dropout(self.alpha)) \n",
    "        self.combined_blocks = nn.Sequential(*combined_blocks)\n",
    "        \n",
    "        self.fc_pro_ligand_1 = nn.Linear(512, 64)\n",
    "        self.fc_pro_ligand_2 = nn.Linear(64, 1)\n",
    "        self.fc_sigmoid = nn.Sigmoid()\n",
    "        self.fc_relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.dropout4 = nn.Dropout(0.4)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "\n",
    "    def _init_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                \n",
    "    def process_PDB(self, pid, pdbs_dir):\n",
    "        X_list, Y_list, Z_list, atomtype_list = read_pdb(pdbs_dir)\n",
    "        return one_hot_protein(atomtype_list)\n",
    "\n",
    "    def batch_process_SMILE(self, ligands):\n",
    "        return one_hot_smiles(ligands)\n",
    "    \n",
    "    def forward(self, protein_input, ligand_input):\n",
    "        '''\n",
    "        protein_input: shape(1200, num_encoding)\n",
    "        ligand_input: shape(200, num_encoding)\n",
    "        '''\n",
    "        \n",
    "        protein_output = self.protein_blocks(protein_input)\n",
    "        ligand_output = self.ligand_blocks(ligand_input)\n",
    "        \n",
    "        protein_ligand = torch.concat([protein_output, ligand_output], dim = -1)\n",
    "        protein_ligand_out = self.combined_blocks(protein_ligand)\n",
    "        \n",
    "        dense1 = torch.empty(1, 5)\n",
    "        \n",
    "        x = self.dropout1(protein_ligand_out)\n",
    "        x = self.fc_pro_ligand_1(x)\n",
    "        x = self.fc_relu(x)\n",
    "        x = self.fc_pro_ligand_2(x)\n",
    "        x = self.fc_sigmoid(x)\n",
    "        dense1[0][0] = x\n",
    "\n",
    "        x = self.dropout2(protein_ligand_out)\n",
    "        x = self.fc_pro_ligand_1(x)\n",
    "        x = self.fc_relu(x)\n",
    "        x = self.fc_pro_ligand_2(x)\n",
    "        x = self.fc_sigmoid(x)\n",
    "        dense1[0][1] = x\n",
    "\n",
    "        x = self.dropout3(protein_ligand_out)\n",
    "        x = self.fc_pro_ligand_1(x)\n",
    "        x = self.fc_relu(x)\n",
    "        x = self.fc_pro_ligand_2(x)\n",
    "        x = self.fc_sigmoid(x)\n",
    "        dense1[0][2] = x\n",
    "        \n",
    "        x = self.dropout4(protein_ligand_out)\n",
    "        x = self.fc_pro_ligand_1(x)\n",
    "        x = self.fc_relu(x)\n",
    "        x = self.fc_pro_ligand_2(x)\n",
    "        x = self.fc_sigmoid(x)\n",
    "        dense1[0][3] = x\n",
    "        \n",
    "        x = self.dropout5(protein_ligand_out)\n",
    "        x = self.fc_pro_ligand_1(x)\n",
    "        x = self.fc_relu(x)\n",
    "        x = self.fc_pro_ligand_2(x)\n",
    "        x = self.fc_sigmoid(x)\n",
    "        dense1[0][4] = x\n",
    "        \n",
    "        out = torch.mean(dense1)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def inference(self, PID, pdbs_dir, centroid, LIDs, ligands):\n",
    "        p = self.process_PDB(PID, pdbs_dir).to(self.device)\n",
    "        l = self.batch_prcoess_SMILE(ligands).to(self.device)\n",
    "        p = self.batch_extend(p, c)\n",
    "        \n",
    "        return self.forward(p, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df_pair, df_ligands, path):\n",
    "        self.df_pair = df_pair\n",
    "        self.df_ligands = df_ligands\n",
    "        self.path = path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_pair)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        pid = self.df_pair[\"PID\"][idx]\n",
    "        lid = self.df_pair[\"LID\"][idx]\n",
    "        target = np.array([self.df_pair[\"target\"][idx]])\n",
    "        \n",
    "        out_ligand = one_hot_smiles(self.df_ligands[self.df_ligands[\"LID\"] == lid][\"Smiles\"].values[0])\n",
    "        X_list, Y_list, Z_list, atomtype_list = read_pdb(f\"{path}/{pid}.pdb\")\n",
    "        out_protein = one_hot_protein(atomtype_list)       \n",
    "        return out_ligand, out_protein, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 3 # 300\n",
    "batch_size = 128 \n",
    "dropout_alpha = 0.5\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current out channels: 8\n",
      "current out channels: 1\n",
      "current out channels: 32\n",
      "current out channels: 1\n",
      "current out channels: 32\n",
      "current out channels: 32\n",
      "current out channels: 16\n",
      "current out channels: 16\n",
      "current out channels: 16\n",
      "current out channels: 64\n",
      "current out channels: 16\n",
      "current out channels: 64\n",
      "current out channels: 64\n",
      "current out channels: 32\n",
      "current out channels: 32\n",
      "current out channels: 64\n",
      "current out channels: 128\n",
      "current out channels: 128\n",
      "current out channels: 64\n",
      "current out channels: 128\n",
      "current out channels: 64\n",
      "current out channels: 8\n",
      "current out channels: 1\n",
      "current out channels: 16\n",
      "current out channels: 1\n",
      "current out channels: 16\n",
      "current out channels: 16\n",
      "current out channels: 16\n",
      "current out channels: 16\n",
      "current out channels: 16\n",
      "current out channels: 64\n",
      "current out channels: 16\n",
      "current out channels: 64\n",
      "current out channels: 64\n",
      "current out channels: 32\n",
      "current out channels: 32\n",
      "current out channels: 32\n",
      "current out channels: 128\n",
      "current out channels: 128\n",
      "current out channels: 32\n",
      "current out channels: 128\n",
      "current out channels: 32\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = MyNet(dropout_alpha, device).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d current output torch.Size([2, 1, 200, 67])\n",
      "conv2d current output torch.Size([2, 1, 200, 67])\n",
      "conv2d current output torch.Size([2, 1, 200, 67])\n",
      "conv2d current output torch.Size([2, 1, 200, 67])\n",
      "conv2d current output torch.Size([2, 1, 200, 67])\n",
      "conv2d current output torch.Size([2, 32, 200, 67])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "max_pool2d(): argument 'padding' (position 4) must be tuple of ints, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-440-5c6d900c5111>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchsummary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m67\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/envs/cs5242/lib/python3.7/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/envs/cs5242/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-437-e646f2029e1c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, protein_input, ligand_input)\u001b[0m\n\u001b[1;32m    122\u001b[0m         '''\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mprotein_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotein_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotein_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mligand_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mligand_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mligand_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/envs/cs5242/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/envs/cs5242/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/envs/cs5242/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             for hook in itertools.chain(\n",
      "\u001b[0;32m<ipython-input-436-3ce8f42a0cbe>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mbranch_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranch_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mbranch_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranch_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mbranch_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mbranch_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer7\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranch_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/envs/cs5242/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             for hook in itertools.chain(\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/envs/cs5242/lib/python3.7/site-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[1;32m    163\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                             self.return_indices)\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/envs/cs5242/lib/python3.7/site-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/envs/cs5242/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    716\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: max_pool2d(): argument 'padding' (position 4) must be tuple of ints, not str"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, [(1, 200, 67), (1, 1200, 21)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(), \"dataset_20220217_2\", \"pdbs\")\n",
    "custom_dataset = CustomDataset(df_train, df_ligands, path)\n",
    "trainloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 200, 67])\n",
      "torch.Size([128, 1, 1200, 21])\n",
      "torch.Size([128, 1])\n",
      "conv2d current output torch.Size([128, 1, 1200, 21])\n",
      "conv2d current output torch.Size([128, 1, 1200, 21])\n",
      "conv2d current output torch.Size([128, 1, 1200, 21])\n",
      "conv2d current output torch.Size([128, 1, 1200, 21])\n",
      "conv2d current output torch.Size([128, 1, 1200, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:03<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d current output torch.Size([128, 32, 1200, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "max_pool2d(): argument 'padding' (position 4) must be tuple of ints, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-441-1dd231dc1814>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotein\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mligand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/envs/cs5242/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-437-e646f2029e1c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, protein_input, ligand_input)\u001b[0m\n\u001b[1;32m    122\u001b[0m         '''\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mprotein_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotein_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotein_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mligand_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mligand_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mligand_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/envs/cs5242/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/envs/cs5242/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/envs/cs5242/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             for hook in itertools.chain(\n",
      "\u001b[0;32m<ipython-input-436-3ce8f42a0cbe>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mbranch_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranch_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mbranch_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranch_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mbranch_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mbranch_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer7\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranch_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/envs/cs5242/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             for hook in itertools.chain(\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/envs/cs5242/lib/python3.7/site-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[1;32m    163\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                             self.return_indices)\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/envs/cs5242/lib/python3.7/site-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/envs/cs5242/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    716\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: max_pool2d(): argument 'padding' (position 4) must be tuple of ints, not str"
     ]
    }
   ],
   "source": [
    "loss_log=[]\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        ligand = data[0].to(device)\n",
    "        protein = data[1].to(device)\n",
    "        target = data[2].to(device)\n",
    "        print(ligand.shape)\n",
    "        print(protein.shape)\n",
    "        print(target.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(protein.float(), ligand.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_log.append(loss.item())\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 128 == 0:\n",
    "            print('epoch {:3d} | {:5d} batches loss: {:.4f}'.format(epoch, i + 1, running_loss/128))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense, Dropout, Activation, Flatten,Reshape,concatenate,LSTM,Bidirectional, Average\n",
    "from keras.layers import Conv2D, MaxPooling2D,Conv1D,MaxPooling1D,AveragePooling2D\n",
    "from keras.layers import Lambda, dot\n",
    "import tensorflow as tf\n",
    "#import string\n",
    "# from keras.utils import multi_gpu_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import numpy as np\n",
    "import os\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "import tensorflow\n",
    "\n",
    "def attention_3d_block(hidden_states,out_shape=128,name='pro_'):\n",
    "    hidden_size = int(hidden_states.shape[2])\n",
    "    # Inside dense layer\n",
    "    #              hidden_states            dot               W            =>           score_first_part\n",
    "    # (batch_size, time_steps, hidden_size) dot (hidden_size, hidden_size) => (batch_size, time_steps, hidden_size)\n",
    "    # W is the trainable weight matrix of attention Luong's multiplicative style score\n",
    "    score_first_part = Dense(hidden_size, use_bias=False, name=name+'attention_score_vec')(hidden_states)\n",
    "    #            score_first_part           dot        last_hidden_state     => attention_weights\n",
    "    # (batch_size, time_steps, hidden_size) dot   (batch_size, hidden_size)  => (batch_size, time_steps)\n",
    "    h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,), name=name+'last_hidden_state')(hidden_states)\n",
    "    score = dot([score_first_part, h_t], [2, 1], name=name+'attention_score')\n",
    "    attention_weights = Activation('softmax', name=name+'attention_weight')(score)\n",
    "    # (batch_size, time_steps, hidden_size) dot (batch_size, time_steps) => (batch_size, hidden_size)\n",
    "    context_vector = dot([hidden_states, attention_weights], [1, 1], name=name+'context_vector')\n",
    "    pre_activation = concatenate([context_vector, h_t], name=name+'attention_output')\n",
    "    attention_vector = Dense(out_shape, use_bias=False, activation='tanh', name=name+'attention_vector')(pre_activation)\n",
    "    return attention_vector\n",
    "\n",
    "def conv2d_bn(x, nb_filter, num_row, num_col,name,\n",
    "              padding='same', strides=(1, 1), use_bias=False):\n",
    "    x = Conv2D(nb_filter, (num_row, num_col),\n",
    "                      name=name,\n",
    "                      strides=strides,\n",
    "                      padding=padding,\n",
    "                      use_bias=use_bias,\n",
    "                      kernel_regularizer=regularizers.l2(0.00004),\n",
    "                      kernel_initializer=initializers.VarianceScaling(scale=2.0, mode='fan_in', distribution='normal', seed=None))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def block_inception(input,filters_1x1, filters_3x3_reduce, filters_3x3,\n",
    "                      filters_5x5_reduce, filters_5x5, filters_pool_proj,layer_name):\n",
    "    branch_0 = conv2d_bn(input, filters_1x1, 1, 1,name=layer_name+'_branch_0')\n",
    "    branch_1 = conv2d_bn(input, filters_3x3_reduce, 1, 1,name=layer_name+'_branch_1_3x3_reduce')\n",
    "    branch_1 = conv2d_bn(branch_1, filters_3x3, 3, 3,name=layer_name+'_branch_1_3x3')\n",
    "    branch_2 = conv2d_bn(input, filters_5x5_reduce, 1, 1,name=layer_name+'_branch_2_5x5_reduce')\n",
    "    branch_2 = conv2d_bn(branch_2, filters_5x5, 3, 3,name=layer_name+'_branch_2_3x3_0')\n",
    "    branch_2 = conv2d_bn(branch_2, filters_5x5, 3, 3,name=layer_name+'_branch_2_3x3_1')\n",
    "    branch_3 = MaxPooling2D((3,3), strides=(1,1), padding='same',name=layer_name+'_branch_3_maxpooling')(input)#AveragePooling2D\n",
    "    branch_3 = conv2d_bn(branch_3, filters_pool_proj, 1, 1,name=layer_name+'_branch_3_pool_proj')\n",
    "    x = concatenate([branch_0, branch_1, branch_2, branch_3], axis=-1,name=layer_name+'_concat')\n",
    "    return x\n",
    "\n",
    "def block_inception_b(input,filters_1x1, filters_5x5_reduce, filters_5x5,\n",
    "                      filters_7x7_reduce, filters_1x7,filters_7x1,filters_pool_proj,layer_name):\n",
    "    branch_0 = conv2d_bn(input, filters_1x1, 1, 1,name=layer_name+'_branch_0')\n",
    "\n",
    "    branch_1 = conv2d_bn(input,filters_7x7_reduce, 1, 1,name=layer_name+'_branch_1_7x7_reduce')\n",
    "    branch_1 = conv2d_bn(branch_1,filters_1x7, 1, 7,name=layer_name+'_branch_1_7x7_0')\n",
    "    branch_1 = conv2d_bn(branch_1,filters_7x1, 7, 1,name=layer_name+'_branch_1_7x7_1')\n",
    "\n",
    "    branch_2 = conv2d_bn(input, filters_5x5_reduce, 1, 1,name=layer_name+'_branch_2_5x5_reduce')\n",
    "    branch_2 = conv2d_bn(branch_2, filters_5x5, 3, 3,name=layer_name+'_branch_2_3x3_0')\n",
    "    branch_2 = conv2d_bn(branch_2, filters_5x5, 3, 3,name=layer_name+'_branch_2_3x3_1')\n",
    "\n",
    "    branch_3 = AveragePooling2D((3,3), strides=(1,1), padding='same')(input)\n",
    "    branch_3 = conv2d_bn(branch_3, filters_pool_proj, 1, 1,name=layer_name+'_branch_3_pool_proj')\n",
    "\n",
    "    x = concatenate([branch_0, branch_1, branch_2, branch_3], axis=-1)#branch_2,\n",
    "    return x\n",
    "\n",
    "def simple_block(input,nb_filter,num_row,num_col,layer_name):\n",
    "    input = Conv2D(nb_filter, (num_row, num_col), padding='same', activation='relu', name=layer_name+'_conv0')(input)\n",
    "    input = Conv2D(nb_filter, (num_row, num_col), padding='same', activation='relu', name=layer_name+'_conv1')(input)\n",
    "    return input\n",
    "\n",
    "def get_model_classification(save_dir,alpha,\n",
    "                                          pro_branch_switch1='',pro_branch_switch2='',\n",
    "                                          pro_branch_switch3='',pro_add_attention=False,\n",
    "                                          comp_branch_switch1='',comp_branch_switch2='',\n",
    "                                          comp_branch_switch3='',comp_add_attention=False,\n",
    "               ):\n",
    "    ###MODEL\n",
    "    ##input\n",
    "    protein_input = Input(shape=(1200, 21, 1), name='protein_input')\n",
    "    comp_input = Input(shape=(200, 67, 1), name='comp_input')\n",
    "    ##protein branch\n",
    "    # layer1\n",
    "    with tf.device('/gpu:0'):\n",
    "        if pro_branch_switch1 == 'inception_block':\n",
    "            pro_layer1 = block_inception(protein_input, filters_1x1=8, filters_3x3_reduce=1, filters_3x3=32,\n",
    "                                         filters_5x5_reduce=1, filters_5x5=32, filters_pool_proj=16,layer_name='pro_layer1')\n",
    "        else:\n",
    "            pro_layer1 = simple_block(protein_input, nb_filter=32, num_row=3, num_col=3, layer_name='pro_layer1')\n",
    "        pro_layer1 = MaxPooling2D(pool_size=(3, 3),padding='same', name='pro_layer1_poll')(pro_layer1)\n",
    "        # layer2\n",
    "        if pro_branch_switch2=='inception_block':\n",
    "            pro_layer2 = block_inception(pro_layer1, filters_1x1=16, filters_3x3_reduce=16, filters_3x3=64,\n",
    "                                         filters_5x5_reduce=16, filters_5x5=64, filters_pool_proj=32,layer_name='pro_layer2')\n",
    "        else:\n",
    "            pro_layer2=simple_block(pro_layer1,nb_filter=64,num_row=3,num_col=3,layer_name='pro_layer2')\n",
    "        pro_layer2 = MaxPooling2D(pool_size=(3, 3), padding='same',name='pro_layer2_poll')(pro_layer2)\n",
    "            # layer3\n",
    "        if pro_branch_switch3=='inception_block':\n",
    "            pro_layer3 = block_inception(pro_layer2, filters_1x1=32, filters_3x3_reduce=64, filters_3x3=128,\n",
    "                                         filters_5x5_reduce=64, filters_5x5=128, filters_pool_proj=64,layer_name='pro_layer3')\n",
    "        elif pro_branch_switch3=='inception_block_b':\n",
    "            pro_layer3 = block_inception_b(pro_layer2, filters_1x1=32, filters_5x5_reduce=64, filters_5x5=128,\n",
    "                                         filters_7x7_reduce=64, filters_1x7=128,filters_7x1=128, filters_pool_proj=64,layer_name='pro_layer3')\n",
    "        else:\n",
    "            pro_layer3 = simple_block(pro_layer2, nb_filter=128, num_row=3, num_col=3, layer_name='pro_layer3')\n",
    "        pro_layer3 = MaxPooling2D(pool_size=(3, 3), padding='same',name='pro_layer3_pool')(pro_layer3)\n",
    "        # layer4\n",
    "        if pro_add_attention:\n",
    "            h_t = Lambda(tf.reshape,output_shape=[45,352,], arguments={'shape': [-1, 45, 352]}, name='pro_convert_to_timestep')(pro_layer3)\n",
    "            pro_layer_tran_result = attention_3d_block(h_t,1024,'pro_')#batch*1024\n",
    "        else:\n",
    "            pro_layer_tran_result = Flatten(name='pro_layer4_flatten')(pro_layer3)\n",
    "            pro_layer_tran_result = Dense(1024, activation='relu', name='pro_layer5_den')(pro_layer_tran_result)\n",
    "        pro_layer_tran_result = Dropout(alpha, name='pro_drop1')(pro_layer_tran_result)\n",
    "    ##compound branch\n",
    "    # layer1\n",
    "    with tf.device('/gpu:1'):\n",
    "        if comp_branch_switch1=='inception_block':\n",
    "            comp_layer1 = block_inception(comp_input, filters_1x1=8, filters_3x3_reduce=1, filters_3x3=16,\n",
    "                                         filters_5x5_reduce=1, filters_5x5=16, filters_pool_proj=16,\n",
    "                                         layer_name='comp_layer1')\n",
    "        else:\n",
    "            comp_layer1 = simple_block(comp_input, 32, 3, 3, 'comp_layer1')\n",
    "        comp_layer1 = MaxPooling2D(pool_size=(2, 2),padding='same', name='comp_layer1_poll')(comp_layer1)\n",
    "        # layer2\n",
    "        if comp_branch_switch2=='inception_block':\n",
    "            comp_layer2 = block_inception(comp_layer1, filters_1x1=16, filters_3x3_reduce=16, filters_3x3=64,\n",
    "                                          filters_5x5_reduce=16, filters_5x5=64, filters_pool_proj=32,layer_name='comp_layer2')\n",
    "        else:\n",
    "            comp_layer2=simple_block(comp_layer1,64,3,3,'comp_layer2')\n",
    "        comp_layer2 = MaxPooling2D(pool_size=(2, 2), padding='same',name='comp_layer2_poll')(comp_layer2)\n",
    "        # layer3\n",
    "        if comp_branch_switch3=='inception_block':\n",
    "            comp_layer3 = block_inception(comp_layer2, filters_1x1=32, filters_3x3_reduce=32, filters_3x3=128,\n",
    "                                          filters_5x5_reduce=32, filters_5x5=128, filters_pool_proj=32,layer_name='comp_layer3')\n",
    "        elif comp_branch_switch3=='inception_block_b':\n",
    "            comp_layer3 = block_inception_b(comp_layer2, filters_1x1=32, filters_5x5_reduce=32, filters_5x5=128,\n",
    "                                           filters_7x7_reduce=32, filters_1x7=128, filters_7x1=128,\n",
    "                                           filters_pool_proj=32, layer_name='comp_layer3')\n",
    "        else:\n",
    "            comp_layer3=simple_block(comp_layer2,128,3,3,'comp_layer3')\n",
    "        comp_layer3 = MaxPooling2D(pool_size=(2, 2), padding='same',name='comp_layer3_pool')(comp_layer3)\n",
    "        # layer4\n",
    "        if comp_add_attention:\n",
    "            h_t = Lambda(tf.reshape,output_shape=[25*8,320,], arguments={'shape': [-1, 25*8, 320]}, name='comp_convert_to_timestep')(comp_layer3)\n",
    "            comp_layer_tran_result = attention_3d_block(h_t,1024,'comp_')#batch*1024\n",
    "        else:\n",
    "            comp_layer_tran_result = Flatten(name='comp_layer4_flatten')(comp_layer3)\n",
    "            comp_layer_tran_result = Dense(640, activation='relu', name='comp_layer5_den')(comp_layer_tran_result)\n",
    "        # layer5\n",
    "        comp_layer_tran_result = Dropout(alpha, name='comp_drop1')(comp_layer_tran_result)\n",
    "    with tf.device('/gpu:2'):\n",
    "        pro_com = keras.layers.concatenate([pro_layer_tran_result, comp_layer_tran_result])\n",
    "        # We stack a deep densely-connected network on top\n",
    "        fc_pro_com = Dense(512, activation='relu', name='den1')(pro_com)\n",
    "        fc_pro_com = Dropout(alpha, name='drop1')(fc_pro_com)\n",
    "        dense1 = []\n",
    "        FC1 = Dense(64, activation='relu')\n",
    "        for p in np.linspace(0.1,0.5, 5):\n",
    "            x = Dropout(p)(fc_pro_com)\n",
    "            x = FC1(x)\n",
    "            x = Dense(1,activation='sigmoid')(x)\n",
    "            dense1.append(x)\n",
    "        class_out = Average()(dense1)\n",
    "    classification_model = Model([protein_input, comp_input],class_out)\n",
    "    plot_model(classification_model, to_file=save_dir + '/model_with_classification.png', show_shapes=True)\n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.x_val,self.y_val = validation_data\n",
    "    def on_epoch_end(self, epoch, log={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.x_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print('\\n ROC_AUC - epoch:%d - AUC score:%.6f \\n' % (epoch+1, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_class(df_pair, df_ligands, path):\n",
    "    x_prot=[]\n",
    "    x_comp=[]\n",
    "    y_=[]\n",
    "    for i in range(len(df_pair)):\n",
    "        pid = df_pair[\"PID\"][i]\n",
    "        lid = df_pair[\"LID\"][i]\n",
    "        target = np.array([df_pair[\"target\"][i]])\n",
    "\n",
    "        out_ligand = one_hot_smiles(df_ligands[df_ligands[\"LID\"] == lid][\"Smiles\"].values[0])\n",
    "        X_list, Y_list, Z_list, atomtype_list = read_pdb(f\"{path}/{pid}.pdb\")\n",
    "        out_protein = one_hot_protein(atomtype_list)  \n",
    "        \n",
    "        x_prot.append(out_protein)\n",
    "        x_comp.append(out_ligand)\n",
    "        y_.append(target)\n",
    "\n",
    "    print('shape:', len(x_prot))\n",
    "    x_prot=np.array(x_prot)\n",
    "    x_prot = x_prot.reshape([-1, 1200, 21, 1])\n",
    "    x_comp=np.array(x_comp)\n",
    "    x_comp = x_comp.reshape([-1, 200, 67, 1])\n",
    "    y_=np.array(y_)\n",
    "    y_=y_.reshape([-1,1])\n",
    "    return x_prot,x_comp,y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_class_generator(df_pair, df_ligands, path, batch_size, ft_flag = False):\n",
    "    x_prot=[]\n",
    "    x_comp=[]\n",
    "    y_=[]\n",
    "    while 1:\n",
    "        df_pair = df_pair.sample(frac=1).reset_index(drop=True)\n",
    "        for i in range(len(df_pair)):\n",
    "            \n",
    "            pid = df_pair[\"PID\"][i]\n",
    "            lid = df_pair[\"LID\"][i]\n",
    "            target = np.array([df_pair[\"target\"][i]])\n",
    "#             print (pid)\n",
    "#             print(lid)\n",
    "\n",
    "            out_ligand = one_hot_smiles(df_ligands[df_ligands[\"LID\"] == lid][\"Smiles\"].values[0])\n",
    "            X_list, Y_list, Z_list, atomtype_list = read_pdb(f\"{path}/{pid}.pdb\")\n",
    "            out_protein = one_hot_protein(atomtype_list)             \n",
    "\n",
    "            x_prot.append(out_protein)\n",
    "            x_comp.append(out_ligand)\n",
    "            y_.append(target)\n",
    "            if len(x_prot)==batch_size:\n",
    "                x_prot=np.array(x_prot)\n",
    "                x_prot = x_prot.reshape([-1, 1200, 21, 1])\n",
    "                x_comp=np.array(x_comp)\n",
    "                x_comp = x_comp.reshape([-1, 200, 67, 1])\n",
    "                y_=np.array(y_)\n",
    "                y_=y_.reshape([-1,1])\n",
    "                if ft_flag:\n",
    "                    yield ({'protein_input':x_prot, 'comp_input':x_comp}, {'average':y_})\n",
    "                else:\n",
    "                    yield (x_prot, x_comp, y_)  \n",
    "                x_prot=[]\n",
    "                x_comp=[]\n",
    "                y_=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n",
      "shape: 2694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/.pyenv/versions/anaconda3-5.2.0/envs/cs5242/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      " 17/739 [..............................] - ETA: 2:27:41 - loss: 0.8321 - accuracy: 0.6521 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      " 34/739 [>.............................] - ETA: 2:23:15 - loss: 0.8304 - accuracy: 0.6473 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      " 62/739 [=>............................] - ETA: 2:16:19 - loss: 0.8241 - accuracy: 0.6501 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      " 80/739 [==>...........................] - ETA: 2:09:45 - loss: 0.8205 - accuracy: 0.6544 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "103/739 [===>..........................] - ETA: 2:03:30 - loss: 0.8175 - accuracy: 0.6571 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "133/739 [====>.........................] - ETA: 1:55:51 - loss: 0.8152 - accuracy: 0.6560 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "167/739 [=====>........................] - ETA: 1:48:30 - loss: 0.8106 - accuracy: 0.6604 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "183/739 [======>.......................] - ETA: 1:44:59 - loss: 0.8097 - accuracy: 0.6598 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "198/739 [=======>......................] - ETA: 1:41:44 - loss: 0.8083 - accuracy: 0.6604 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "215/739 [=======>......................] - ETA: 1:38:05 - loss: 0.8074 - accuracy: 0.6600 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "246/739 [========>.....................] - ETA: 1:31:29 - loss: 0.8045 - accuracy: 0.6617 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "259/739 [=========>....................] - ETA: 1:28:50 - loss: 0.8037 - accuracy: 0.6619 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "304/739 [===========>..................] - ETA: 1:19:50 - loss: 0.8005 - accuracy: 0.6628 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "324/739 [============>.................] - ETA: 1:15:57 - loss: 0.7992 - accuracy: 0.6629 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "353/739 [=============>................] - ETA: 1:10:23 - loss: 0.7974 - accuracy: 0.6633 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "378/739 [==============>...............] - ETA: 1:05:37 - loss: 0.7961 - accuracy: 0.6632 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "411/739 [===============>..............] - ETA: 59:26 - loss: 0.7941 - accuracy: 0.6633 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "429/739 [================>.............] - ETA: 56:08 - loss: 0.7928 - accuracy: 0.6641 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "452/739 [=================>............] - ETA: 51:53 - loss: 0.7918 - accuracy: 0.6635 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "458/739 [=================>............] - ETA: 50:47 - loss: 0.7914 - accuracy: 0.6635 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "521/739 [====================>.........] - ETA: 39:11 - loss: 0.7880 - accuracy: 0.6638 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "536/739 [====================>.........] - ETA: 36:29 - loss: 0.7872 - accuracy: 0.6641 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "556/739 [=====================>........] - ETA: 32:51 - loss: 0.7861 - accuracy: 0.6641 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "559/739 [=====================>........] - ETA: 32:18 - loss: 0.7859 - accuracy: 0.6643 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "592/739 [=======================>......] - ETA: 26:19 - loss: 0.7842 - accuracy: 0.6644 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "600/739 [=======================>......] - ETA: 24:52 - loss: 0.7838 - accuracy: 0.6644 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "661/739 [=========================>....] - ETA: 13:58 - loss: 0.7807 - accuracy: 0.6647 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "682/739 [==========================>...] - ETA: 10:12 - loss: 0.7798 - accuracy: 0.6645 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "691/739 [===========================>..] - ETA: 8:35 - loss: 0.7794 - accuracy: 0.6647 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "730/739 [============================>.] - ETA: 1:36 - loss: 0.7777 - accuracy: 0.6648 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "739/739 [==============================] - ETA: 0s - loss: 0.7773 - accuracy: 0.6646 \n",
      " ROC_AUC - epoch:1 - AUC score:0.496042 \n",
      "\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.74314, saving model to ./first_model_best_model.hdf5\n",
      "739/739 [==============================] - 8064s 11s/step - loss: 0.7773 - accuracy: 0.6646 - val_loss: 0.7431 - val_accuracy: 0.6667\n",
      "Epoch 2/300\n",
      "  3/739 [..............................] - ETA: 2:17:45 - loss: 0.7599 - accuracy: 0.6250 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "  8/739 [..............................] - ETA: 2:12:16 - loss: 0.7491 - accuracy: 0.6533 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      " 49/739 [>.............................] - ETA: 2:01:14 - loss: 0.7411 - accuracy: 0.6680 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      " 91/739 [==>...........................] - ETA: 1:54:04 - loss: 0.7390 - accuracy: 0.6697 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      " 98/739 [==>...........................] - ETA: 1:52:58 - loss: 0.7393 - accuracy: 0.6688 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "111/739 [===>..........................] - ETA: 1:50:33 - loss: 0.7395 - accuracy: 0.6672 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "154/739 [=====>........................] - ETA: 1:43:03 - loss: 0.7377 - accuracy: 0.6683 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "176/739 [======>.......................] - ETA: 1:38:56 - loss: 0.7366 - accuracy: 0.6692 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "223/739 [========>.....................] - ETA: 1:30:17 - loss: 0.7357 - accuracy: 0.6677 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "237/739 [========>.....................] - ETA: 1:27:42 - loss: 0.7354 - accuracy: 0.6675 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "255/739 [=========>....................] - ETA: 1:24:33 - loss: 0.7351 - accuracy: 0.6669 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "261/739 [=========>....................] - ETA: 1:23:28 - loss: 0.7349 - accuracy: 0.6669 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "299/739 [===========>..................] - ETA: 1:16:56 - loss: 0.7341 - accuracy: 0.6660 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      " Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "347/739 [=============>................] - ETA: 1:08:40 - loss: 0.7323 - accuracy: 0.6671 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "380/739 [==============>...............] - ETA: 1:04:49 - loss: 0.7314 - accuracy: 0.6668 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "400/739 [===============>..............] - ETA: 1:01:35 - loss: 0.7306 - accuracy: 0.6674 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "412/739 [===============>..............] - ETA: 59:23 - loss: 0.7304 - accuracy: 0.6672 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "449/739 [=================>............] - ETA: 52:33 - loss: 0.7296 - accuracy: 0.6667 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "458/739 [=================>............] - ETA: 50:55 - loss: 0.7292 - accuracy: 0.6671 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "490/739 [==================>...........] - ETA: 45:36 - loss: 0.7286 - accuracy: 0.6668 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "506/739 [===================>..........] - ETA: 43:25 - loss: 0.7281 - accuracy: 0.6669 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "537/739 [====================>.........] - ETA: 37:46 - loss: 0.7273 - accuracy: 0.6668 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "553/739 [=====================>........] - ETA: 34:58 - loss: 0.7269 - accuracy: 0.6671 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "593/739 [=======================>......] - ETA: 27:24 - loss: 0.7259 - accuracy: 0.6668 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "606/739 [=======================>......] - ETA: 24:59 - loss: 0.7257 - accuracy: 0.6667 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "647/739 [=========================>....] - ETA: 17:19 - loss: 0.7247 - accuracy: 0.6668 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "661/739 [=========================>....] - ETA: 14:41 - loss: 0.7244 - accuracy: 0.6667 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "702/739 [===========================>..] - ETA: 6:57 - loss: 0.7232 - accuracy: 0.6668 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "707/739 [===========================>..] - ETA: 6:01 - loss: 0.7230 - accuracy: 0.6669 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "739/739 [==============================] - ETA: 0s - loss: 0.7223 - accuracy: 0.6665 \n",
      " ROC_AUC - epoch:2 - AUC score:0.369238 \n",
      "\n",
      "\n",
      "Epoch 2: val_loss improved from 0.74314 to 0.70941, saving model to ./first_model_best_model.hdf5\n",
      "739/739 [==============================] - 8512s 12s/step - loss: 0.7223 - accuracy: 0.6665 - val_loss: 0.7094 - val_accuracy: 0.6667\n",
      "Epoch 3/300\n",
      " 15/739 [..............................] - ETA: 2:26:17 - loss: 0.6999 - accuracy: 0.6635 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      " 33/739 [>.............................] - ETA: 2:18:33 - loss: 0.7003 - accuracy: 0.6641 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      " 59/739 [=>............................] - ETA: 2:11:47 - loss: 0.6961 - accuracy: 0.6694 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      " 73/739 [=>............................] - ETA: 2:08:47 - loss: 0.6947 - accuracy: 0.6705 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "119/739 [===>..........................] - ETA: 1:59:34 - loss: 0.6928 - accuracy: 0.6691 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "123/739 [===>..........................] - ETA: 1:58:45 - loss: 0.6927 - accuracy: 0.6691 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "152/739 [=====>........................] - ETA: 1:52:56 - loss: 0.6928 - accuracy: 0.6677 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "188/739 [======>.......................] - ETA: 1:46:15 - loss: 0.6921 - accuracy: 0.6675 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "199/739 [=======>......................] - ETA: 1:44:14 - loss: 0.6918 - accuracy: 0.6672 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "235/739 [========>.....................] - ETA: 1:37:27 - loss: 0.6906 - accuracy: 0.6675 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "242/739 [========>.....................] - ETA: 1:36:08 - loss: 0.6906 - accuracy: 0.6674 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "279/739 [==========>...................] - ETA: 1:29:27 - loss: 0.6899 - accuracy: 0.6669 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "291/739 [==========>...................] - ETA: 1:27:13 - loss: 0.6893 - accuracy: 0.6673 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "312/739 [===========>..................] - ETA: 1:23:20 - loss: 0.6895 - accuracy: 0.6663 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "345/739 [=============>................] - ETA: 1:18:06 - loss: 0.6885 - accuracy: 0.6669 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "355/739 [=============>................] - ETA: 1:16:17 - loss: 0.6884 - accuracy: 0.6669 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "418/739 [===============>..............] - ETA: 1:03:59 - loss: 0.6870 - accuracy: 0.6671 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "431/739 [================>.............] - ETA: 1:01:20 - loss: 0.6866 - accuracy: 0.6673 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "441/739 [================>.............] - ETA: 59:17 - loss: 0.6865 - accuracy: 0.6671 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "465/739 [=================>............] - ETA: 54:24 - loss: 0.6862 - accuracy: 0.6668 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "499/739 [===================>..........] - ETA: 47:30 - loss: 0.6854 - accuracy: 0.6668 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "504/739 [===================>..........] - ETA: 46:27 - loss: 0.6852 - accuracy: 0.6669 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "554/739 [=====================>........] - ETA: 36:15 - loss: 0.6839 - accuracy: 0.6673 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      " Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "593/739 [=======================>......] - ETA: 28:28 - loss: 0.6835 - accuracy: 0.6668 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "607/739 [=======================>......] - ETA: 25:41 - loss: 0.6830 - accuracy: 0.6669 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "659/739 [=========================>....] - ETA: 15:25 - loss: 0.6820 - accuracy: 0.6668 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "662/739 [=========================>....] - ETA: 14:50 - loss: 0.6820 - accuracy: 0.6667 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "708/739 [===========================>..] - ETA: 5:58 - loss: 0.6808 - accuracy: 0.6670 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "714/739 [===========================>..] - ETA: 4:49 - loss: 0.6806 - accuracy: 0.6670 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "739/739 [==============================] - ETA: 0s - loss: 0.6802 - accuracy: 0.6669 \n",
      " ROC_AUC - epoch:3 - AUC score:0.343216 \n",
      "\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.70941\n",
      "739/739 [==============================] - 8723s 12s/step - loss: 0.6802 - accuracy: 0.6669 - val_loss: 0.7587 - val_accuracy: 0.6667\n",
      "Epoch 4/300\n",
      " 17/739 [..............................] - ETA: 2:27:41 - loss: 0.6555 - accuracy: 0.6765 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      " 23/739 [..............................] - ETA: 2:24:34 - loss: 0.6602 - accuracy: 0.6678 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      " 62/739 [=>............................] - ETA: 2:29:28 - loss: 0.6640 - accuracy: 0.6660 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      " 73/739 [=>............................] - ETA: 2:27:02 - loss: 0.6623 - accuracy: 0.6667 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "101/739 [===>..........................] - ETA: 2:21:13 - loss: 0.6641 - accuracy: 0.6627 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "122/739 [===>..........................] - ETA: 2:16:49 - loss: 0.6622 - accuracy: 0.6657 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "153/739 [=====>........................] - ETA: 2:11:21 - loss: 0.6597 - accuracy: 0.6663 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "163/739 [=====>........................] - ETA: 2:08:35 - loss: 0.6593 - accuracy: 0.6665 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "211/739 [=======>......................] - ETA: 2:02:00 - loss: 0.6576 - accuracy: 0.6658 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "235/739 [========>.....................] - ETA: 1:54:32 - loss: 0.6566 - accuracy: 0.6662 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "255/739 [=========>....................] - ETA: 1:49:01 - loss: 0.6551 - accuracy: 0.6671 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "269/739 [=========>....................] - ETA: 1:44:57 - loss: 0.6542 - accuracy: 0.6669 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "319/739 [===========>..................] - ETA: 1:33:37 - loss: 0.6511 - accuracy: 0.6667 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "325/739 [============>.................] - ETA: 1:32:51 - loss: 0.6507 - accuracy: 0.6670 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "342/739 [============>.................] - ETA: 1:31:04 - loss: 0.6496 - accuracy: 0.6671 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "344/739 [============>.................] - ETA: 1:30:32 - loss: 0.6493 - accuracy: 0.6672 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "394/739 [==============>...............] - ETA: 1:22:17 - loss: 0.6464 - accuracy: 0.6668 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "432/739 [================>.............] - ETA: 1:14:40 - loss: 0.6436 - accuracy: 0.6671 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "444/739 [=================>............] - ETA: 1:11:58 - loss: 0.6426 - accuracy: 0.6671 Cc1c2n3c(c1CCC(=O)O)C=C4C(=C(C5=Cc6c(c(c7n6[Co]3(N45)N8C(=C7)C(=C(C8=C2)CCC(=O)O)C)CCC(=O)O)C)CCC(=O)O)C exits not in SMISET character  \n",
      "450/739 [=================>............] - ETA: 1:10:43 - loss: 0.6423 - accuracy: 0.6670"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "save_dir = \".\"\n",
    "model_name = \"first_model\"\n",
    "alpha = 0.5\n",
    "batch_size = 128\n",
    "epochs = 300\n",
    "lr = 0.0001\n",
    "patience = 10 \n",
    "path = os.path.join(os.getcwd(), \"dataset_20220217_2\", \"pdbs\")\n",
    "\n",
    "model = get_model_classification(save_dir, alpha,                 \n",
    "                    pro_branch_switch1 = 'inception_block', pro_branch_switch2 = 'inception_block',\n",
    "                    pro_branch_switch3='inception_block_b', pro_add_attention = False,\n",
    "                    comp_branch_switch1 = 'inception_block', comp_branch_switch2 = 'inception_block',\n",
    "                    comp_branch_switch3 = 'inception_block_b', comp_add_attention = False)\n",
    "\n",
    "validation_x_prot, validation_x_comp, validation_y=read_class(df_test, df_ligands, path)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=lr)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "bestfile = save_dir + \"/%s_best_model.hdf5\" % model_name\n",
    "checkpoint = ModelCheckpoint(bestfile, monitor='val_loss', verbose=1, save_best_only=True,\n",
    "                             mode='min')    \n",
    "# AUC\n",
    "RocAuc = RocAucEvaluation(validation_data=([validation_x_prot, validation_x_comp],\n",
    "                                               validation_y), interval=1)\n",
    "\n",
    "path = os.path.join(os.getcwd(), \"dataset_20220217_2\", \"pdbs\")\n",
    "history = model.fit(read_class_generator(df_train, df_ligands, path, batch_size, ft_flag = True),\n",
    "                    steps_per_epoch=739,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=([validation_x_prot, validation_x_comp], validation_y),\n",
    "                    callbacks=[RocAuc,early_stopping, checkpoint]\n",
    "                    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5242",
   "language": "python",
   "name": "cs5242"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
